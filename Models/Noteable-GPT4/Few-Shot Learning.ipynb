{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "73919543-4a99-4205-95fe-ba32277dbbdc"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small"
  },
  "cells": [
    {
      "id": "215f8cec",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "",
      "outputs": []
    },
    {
      "id": "70471256-37a2-49ed-b168-8d88dfabd70a",
      "cell_type": "markdown",
      "source": "# Small Business Loan Approval Analysis\n\nIn this notebook, we will analyze the data related to small business loan approvals. Our goal is to understand the factors that influence loan approvals and use machine learning models to predict which businesses should be granted loans.\n\nThe analysis will be structured as follows:\n1. **Data Preprocessing**: We'll start by loading and cleaning the data, handling missing values, and ensuring it's in the right format for analysis.\n2. **Feature Engineering**: We'll create new features that might be relevant for our analysis and predictions.\n3. **Exploratory Data Analysis (EDA)**: We'll visualize the data to understand the trends, patterns, and relationships between different features.\n4. **Model Building**: We'll train machine learning models to predict loan approvals.\n5. **Explainable AI (XAI)**: We'll use SHAP values to understand the importance of different features in our model's predictions.\n\nLet's begin!",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "1d563731-cedc-46c2-b04a-9c5b481f2634",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "7d4f20f5-d458-4d13-95fe-6b0f9424100c"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:15:31.254463+00:00",
          "start_time": "2023-09-03T03:15:23.388303+00:00"
        },
        "datalink": {
          "d27652b5-1547-49ad-96a7-79abe4c3b419": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 27,
              "orig_num_rows": 5,
              "orig_size_bytes": 1120,
              "truncated_num_cols": 27,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 1120,
              "truncated_string_columns": []
            },
            "display_id": "d27652b5-1547-49ad-96a7-79abe4c3b419",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-03T03:15:31.088434",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_0ac39c1bb3ff4c0e8620b5e18a3613a0"
          }
        }
      },
      "execution_count": null,
      "source": "# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting display options for pandas dataframe\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Loading the datasets\nfile0 = pd.read_csv('file0.csv')\nfile1 = pd.read_csv('file1.csv')\nfile2 = pd.read_csv('file2.csv')\n\n# Concatenating the datasets to form a single dataframe\ndf = pd.concat([file0, file1, file2], ignore_index=True)\n\n# Displaying the first few rows of the dataframe\ndf.head()",
      "outputs": []
    },
    {
      "id": "e2a62833-5859-44ec-813f-0a0c59afa00b",
      "cell_type": "markdown",
      "source": "## Data Preprocessing\n\nFrom the initial glance at the data, we can observe the following:\n\n1. Some columns have data in object format which might be due to the presence of special characters like `$`. We'll need to convert these columns to appropriate numeric formats.\n2. The `ChgOffDate` column has 100% missing values in the displayed rows. We'll need to further investigate this column.\n3. Some columns like `ApprovalDate`, `DisbursementDate`, and `ChgOffDate` are dates and might need to be converted to datetime format.\n\nLet's start by handling these issues.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "aef38862-8338-4f4f-9840-861bf4d9a41b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bec1cc8f-aa5e-42e3-be00-5c65ca51dd83"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:16:13.092599+00:00",
          "start_time": "2023-09-03T03:16:02.279234+00:00"
        },
        "datalink": {
          "155f5bdb-f6e4-4b5c-b57f-0ed0d0b10537": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 1,
              "orig_num_rows": 27,
              "orig_size_bytes": 432,
              "truncated_num_cols": 1,
              "truncated_num_rows": 27,
              "truncated_size_bytes": 432,
              "truncated_string_columns": []
            },
            "display_id": "155f5bdb-f6e4-4b5c-b57f-0ed0d0b10537",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-03T03:16:12.934529",
            "user_variable_name": "missing_percentage",
            "variable_name": "missing_percentage"
          }
        }
      },
      "execution_count": null,
      "source": "# Converting columns with $ and , to float type\nmoney_columns = ['DisbursementGross', 'BalanceGross', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv']\nfor col in money_columns:\n    df[col] = df[col].str.replace(',', '').str.replace('$', '').astype(float)\n\n# Converting date columns to datetime format\ndate_columns = ['ApprovalDate', 'DisbursementDate', 'ChgOffDate']\nfor col in date_columns:\n    df[col] = pd.to_datetime(df[col], errors='coerce')\n\n# Checking the percentage of missing values for each column\nmissing_percentage = df.isnull().mean() * 100\nmissing_percentage",
      "outputs": []
    },
    {
      "id": "7c910ed5-d898-4fe0-ab28-ac95f958b2ef",
      "cell_type": "markdown",
      "source": "From the output, we can observe that:\n\n1. The `DisbursementDate` column has about 0.26% missing values.\n2. The `BankState` column has about 0.17% missing values.\n3. The `Name` column has a very small percentage of missing values (0.0015%).\n\nGiven the small percentage of missing values, we can choose to either impute these values or drop the rows with missing values. For simplicity, we'll drop the rows with missing values for now.\n\nNext, we'll proceed with feature engineering.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "08f775b5-0376-4ab2-9d33-a4cf84758297",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "743143e3-5dde-4d3f-84fa-7489ae670cec"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:16:43.339790+00:00",
          "start_time": "2023-09-03T03:16:42.793302+00:00"
        }
      },
      "execution_count": null,
      "source": "# Dropping rows with missing values\ndf.dropna(subset=['DisbursementDate', 'BankState', 'Name'], inplace=True)\n\n# Resetting the index\ndf.reset_index(drop=True, inplace=True)\n\n# Checking the shape of the dataframe after dropping missing values\ndf.shape",
      "outputs": []
    },
    {
      "id": "2d71329a-b2e8-40e7-92b0-0c3dab5009d4",
      "cell_type": "markdown",
      "source": "## Feature Engineering\n\nFeature engineering involves creating new features or modifying existing features to improve the performance of machine learning models. Given the dataset and the provided examples, we can create the following new features:\n\n1. **Industry**: Based on the first two digits of the `NAICS` feature, we can categorize businesses into their respective industries.\n2. **RealEstate**: Based on the `Term` feature, if the term is greater than or equal to 240 months, it indicates that the loan is related to real estate.\n\nLet's create these features.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "73c34e5e-4a5f-4bf6-8fae-a6d0c1e52a4c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f8234e3b-8df1-41a3-822c-ee00c4860a94"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:17:36.891600+00:00",
          "start_time": "2023-09-03T03:17:29.361113+00:00"
        },
        "datalink": {
          "61a8608d-c462-479c-8c83-95f3fdfbffb6": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 4,
              "orig_num_rows": 5,
              "orig_size_bytes": 200,
              "truncated_num_cols": 4,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 200,
              "truncated_string_columns": []
            },
            "display_id": "61a8608d-c462-479c-8c83-95f3fdfbffb6",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-03T03:17:36.731995",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_71fe024998e24fd2a4caac2a4d5c6a0c"
          }
        }
      },
      "execution_count": null,
      "source": "# Creating the 'Industry' feature based on the first two digits of 'NAICS'\nindustry_mapping = {\n    '11': 'Agriculture, forestry, fishing, and hunting',\n    '21': 'Mining, quarrying, and oil and gas extraction',\n    '22': 'Utilities',\n    '23': 'Construction',\n    '31': 'Manufacturing', '32': 'Manufacturing', '33': 'Manufacturing',\n    '42': 'Wholesale trade',\n    '44': 'Retail trade', '45': 'Retail trade',\n    '48': 'Transportation and warehousing', '49': 'Transportation and warehousing',\n    '51': 'Information',\n    '52': 'Finance and insurance',\n    '53': 'Real estate and rental and leasing',\n    '54': 'Professional, scientific, and technical services',\n    '55': 'Management of companies and enterprises',\n    '56': 'Administrative and support and waste management and remediation services',\n    '61': 'Educational services',\n    '62': 'Health care and social assistance',\n    '71': 'Arts, entertainment, and recreation',\n    '72': 'Accommodation and food services',\n    '81': 'Other services except public administration',\n    '92': 'Public administration'\n}\ndf['Industry'] = df['NAICS'].astype(str).str[:2].map(industry_mapping)\n\n# Creating the 'RealEstate' feature based on the 'Term' feature\ndf['RealEstate'] = np.where(df['Term'] >= 240, 1, 0)\n\n# Displaying the first few rows of the dataframe after feature engineering\ndf[['NAICS', 'Industry', 'Term', 'RealEstate']].head()",
      "outputs": []
    },
    {
      "id": "726bb5cc-94f6-4759-bd00-28b9b7902068",
      "cell_type": "markdown",
      "source": "The new features `Industry` and `RealEstate` have been successfully created. We can observe that some rows have `NAICS` as 0, which results in NaN values for the `Industry` feature. We'll handle these NaN values by categorizing them as 'Unknown'.\n\nNext, we'll proceed with exploratory data analysis (EDA) to visualize and understand the data better.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "07c2da08-55a5-4e41-9169-02e398fb5180",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "6fda3880-4744-4f83-a977-1364ef37e82b"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:18:10.754815+00:00",
          "start_time": "2023-09-03T03:18:03.799451+00:00"
        },
        "datalink": {
          "d01ec629-46ca-4a87-98f0-a7c5a0acf5d4": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 1,
              "orig_num_rows": 21,
              "orig_size_bytes": 336,
              "truncated_num_cols": 1,
              "truncated_num_rows": 21,
              "truncated_size_bytes": 336,
              "truncated_string_columns": []
            },
            "display_id": "d01ec629-46ca-4a87-98f0-a7c5a0acf5d4",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-03T03:18:10.597159",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_6ac38d539b274e00baf94f9dc721d7cf"
          }
        }
      },
      "execution_count": null,
      "source": "# Handling NaN values in 'Industry' column\ndf['Industry'].fillna('Unknown', inplace=True)\n\n# Checking the unique values in 'Industry' column\ndf['Industry'].value_counts()",
      "outputs": []
    },
    {
      "id": "97ffaf4d-2809-42f3-a4ae-a8501c76aecb",
      "cell_type": "markdown",
      "source": "## Exploratory Data Analysis (EDA)\n\nEDA helps us understand the data by visualizing and summarizing its main characteristics. This step is crucial before diving into machine learning or statistical modeling as it allows us to understand the underlying structure of the data, identify outliers, and detect anomalies.\n\nLet's start by visualizing the distribution of loans across different industries.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "f019de6d-a5e1-4578-935c-796cbfdbd449",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9d6f12d3-5b24-4a1f-9002-9beeb696f9cb"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:18:43.584541+00:00",
          "start_time": "2023-09-03T03:18:42.921182+00:00"
        }
      },
      "execution_count": null,
      "source": "# Visualizing the distribution of loans across different industries\nplt.figure(figsize=(15, 10))\nindustry_counts = df['Industry'].value_counts()\nsns.barplot(y=industry_counts.index, x=industry_counts.values, palette='viridis')\nplt.xlabel('Number of Loans')\nplt.ylabel('Industry')\nplt.title('Distribution of Loans Across Different Industries')\nplt.show()",
      "outputs": []
    },
    {
      "id": "406e650c-e5ed-4a78-84ae-cba43f026998",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e59a23e4-ee68-436c-b665-9dd38d3bcfde"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:22:20.675319+00:00",
          "start_time": "2023-09-03T03:22:13.372744+00:00"
        },
        "datalink": {
          "923d2ce2-e727-453e-a105-77f42718c84a": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 4,
              "orig_num_rows": 5,
              "orig_size_bytes": 200,
              "truncated_num_cols": 4,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 200,
              "truncated_string_columns": []
            },
            "display_id": "923d2ce2-e727-453e-a105-77f42718c84a",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-09-03T03:22:20.516343",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_8a60d377ac99436e90eba77b522c3d41"
          }
        }
      },
      "execution_count": null,
      "source": "# Continuing the creation of the 'RealEstate' feature based on the 'Term' feature\ndf['RealEstate'] = np.where(df['Term'] >= 240, 1, 0)\n\n# Displaying the first few rows of the dataframe to check the new features\ndf[['NAICS', 'Industry', 'Term', 'RealEstate']].head()",
      "outputs": []
    },
    {
      "id": "a9e85163-59ac-4e1e-90d4-3880ae007c34",
      "cell_type": "markdown",
      "source": "The new features, `Industry` and `RealEstate`, have been successfully created. We can observe that some rows have 'Unknown' as their industry. This is because the `NAICS` code for these rows is either missing or doesn't match the provided mapping. We'll handle these 'Unknown' values in the next steps.\n\nNext, let's proceed with Exploratory Data Analysis (EDA) to visualize and understand the data better.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "69e34e83-2efb-43d0-9526-c1503ad013c0",
      "cell_type": "markdown",
      "source": "## Exploratory Data Analysis (EDA)\n\nEDA helps in understanding the data by visualizing and summarizing its main characteristics. We'll start by visualizing the distribution of loan approvals based on different features to identify patterns and trends.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "4dae322b-065c-45e6-bdc0-d45c95f0a179",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "928a3be1-0c4b-4a76-b1a0-47ea003868a0"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:22:58.740948+00:00",
          "start_time": "2023-09-03T03:22:57.616939+00:00"
        }
      },
      "execution_count": null,
      "source": "# Plotting the distribution of loan approvals based on the 'Industry' feature\nplt.figure(figsize=(15, 8))\nsns.countplot(y='Industry', hue='MIS_Status', data=df, order=df['Industry'].value_counts().index)\nplt.title('Distribution of Loan Approvals based on Industry')\nplt.xlabel('Count')\nplt.ylabel('Industry')\nplt.legend(title='Loan Status')\nplt.tight_layout()\nplt.show()",
      "outputs": []
    },
    {
      "id": "e56f1fbf-fc2f-4eff-8176-23453195132a",
      "cell_type": "markdown",
      "source": "From the above visualization, we can observe the following:\n\n1. **Retail Trade** and **Professional, scientific, and technical services** are the top industries seeking loans.\n2. Most of the loans across all industries are paid in full (`PIF`). However, there are still a significant number of loans that are charged off (`CHGOFF`), especially in the **Retail Trade** and **Construction** industries.\n3. The **Unknown** category has a high number of loans, indicating that there are many businesses with NAICS codes that don't match the provided mapping. We might need to further investigate or categorize these businesses in future analyses.\n\nNext, let's visualize the distribution of loan approvals based on the `RealEstate` feature.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "4ece012d-f521-4b1a-a287-ca774aaf0bdd",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b28ac77d-1f9b-42da-a017-3b4528ed7e9d"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:23:29.162064+00:00",
          "start_time": "2023-09-03T03:23:28.519214+00:00"
        }
      },
      "execution_count": null,
      "source": "# Plotting the distribution of loan approvals based on the 'RealEstate' feature\nplt.figure(figsize=(10, 6))\nsns.countplot(x='RealEstate', hue='MIS_Status', data=df)\nplt.title('Distribution of Loan Approvals based on RealEstate')\nplt.xlabel('RealEstate (0: Non-RealEstate, 1: RealEstate)')\nplt.ylabel('Count')\nplt.legend(title='Loan Status')\nplt.tight_layout()\nplt.show()",
      "outputs": []
    },
    {
      "id": "2ed011c7-307e-4b4e-b137-4eb99fa891aa",
      "cell_type": "markdown",
      "source": "From the visualization, we can observe the following:\n\n1. A majority of the loans are not related to real estate (`RealEstate = 0`), but there's still a significant number of real estate-related loans.\n2. For both real estate and non-real estate loans, most of the loans are paid in full (`PIF`). However, the proportion of charged off loans (`CHGOFF`) seems slightly higher for non-real estate loans.\n\nThis indicates that real estate loans might be slightly less risky, possibly because the underlying asset (real estate) can be used as collateral.\n\nNext, we'll proceed with building machine learning models to predict loan approvals.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "76c8ba5f-7876-491a-8a9e-477c4f8b8731",
      "cell_type": "markdown",
      "source": "## Model Building\n\nWe'll start by preparing the data for modeling. This involves:\n\n1. Encoding categorical variables.\n2. Splitting the data into training and testing sets.\n3. Scaling the features.\n\nAfter preparing the data, we'll train a few machine learning models and evaluate their performance. Given the CPU limit, we'll select models that are computationally efficient.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "c2bfa8e8-30b7-467a-826b-28a3320b5eb8",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1389e8ea-78c5-4eca-9e59-37b33bbb58f1"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:24:24.661026+00:00",
          "start_time": "2023-09-03T03:24:22.548278+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Selecting relevant features for modeling\nfeatures = ['City', 'State', 'Zip', 'Bank', 'BankState', 'NAICS', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist',\n            'CreateJob', 'RetainedJob', 'UrbanRural', 'RevLineCr', 'LowDoc', 'DisbursementGross',\n            'BalanceGross', 'GrAppv', 'SBA_Appv', 'Industry', 'RealEstate']\ntarget = 'MIS_Status'\n\n# Encoding categorical variables\nlabel_encoders = {}\nfor col in df[features].select_dtypes(include=['object']).columns:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col].astype(str))\n    label_encoders[col] = le\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n\n# Scaling the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train.shape, X_test.shape",
      "outputs": []
    },
    {
      "id": "95a29e53-fa7f-4c20-898a-1354865a9fdb",
      "cell_type": "markdown",
      "source": "The data has been successfully preprocessed and split into training and testing sets. We have 716,216 samples in the training set and 179,055 samples in the testing set, with each sample having 21 features.\n\nNext, we'll train a few machine learning models. Given the CPU limit, we'll start with logistic regression, which is computationally efficient. We'll then evaluate its performance on the testing set.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "95cf371a-267e-425d-a825-0c17cf94c895",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1747eb95-2738-4e27-8a46-6d9ad0240ea8"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:24:54.413681+00:00",
          "start_time": "2023-09-03T03:24:53.590025+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Initializing and training the logistic regression model\nlogreg = LogisticRegression(max_iter=1000, random_state=42)\nlogreg.fit(X_train, y_train)\n\n# Predicting on the testing set\ny_pred = logreg.predict(X_test)\n\n# Evaluating the model's performance\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\naccuracy, classification_rep",
      "outputs": []
    },
    {
      "id": "08d38121-ce8d-40be-97c1-61efdd1b33ed",
      "cell_type": "markdown",
      "source": "It seems there's an error due to the presence of NaN values in the input data. We'll need to investigate and handle these NaN values before proceeding with the model training.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "2d6dbd10-2d44-41e0-a99e-c956ba3fa9d7",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b1ab804d-c4af-43de-beab-cd2fdc77a6dd"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:25:17.494621+00:00",
          "start_time": "2023-09-03T03:25:17.322310+00:00"
        }
      },
      "execution_count": null,
      "source": "# Checking for NaN values in the training set\nnan_columns = X_train.columns[np.any(np.isnan(X_train), axis=0)]\nnan_columns",
      "outputs": []
    },
    {
      "id": "b6681b3c-9227-495c-b794-50e61d9bb0fb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "69e72092-2051-43a9-94ce-2e3406d0d11c"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:28:30.953913+00:00",
          "start_time": "2023-09-03T03:28:30.691778+00:00"
        }
      },
      "execution_count": null,
      "source": "# Continuing the training of the logistic regression model\nlogreg.fit(X_train, y_train)\n\n# Predicting on the test set\ny_pred = logreg.predict(X_test)\n\n# Evaluating the model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\naccuracy, classification_rep",
      "outputs": []
    },
    {
      "id": "da802baf-803e-44b2-9eef-165c1f5d1774",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "00b2a0c2-fa54-4644-ba31-592a42e37b7e"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:28:51.092920+00:00",
          "start_time": "2023-09-03T03:28:50.921462+00:00"
        }
      },
      "execution_count": null,
      "source": "# Checking for NaN values in the training and testing sets\nnan_train = pd.DataFrame(X_train).isna().sum().sum()\nnan_test = pd.DataFrame(X_test).isna().sum().sum()\n\nnan_train, nan_test",
      "outputs": []
    },
    {
      "id": "14d96643-f4ba-463b-a942-ac3b51ee3137",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "19abed7e-bebc-4214-96f6-1ca0f81a42c3"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:29:19.753242+00:00",
          "start_time": "2023-09-03T03:29:19.334634+00:00"
        }
      },
      "execution_count": null,
      "source": "from sklearn.impute import SimpleImputer\n\n# Imputing NaN values with the mean of the respective columns\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.transform(X_test)\n\n# Retraining the logistic regression model\nlogreg.fit(X_train, y_train)\n\n# Predicting on the test set\ny_pred = logreg.predict(X_test)\n\n# Evaluating the model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\naccuracy, classification_rep",
      "outputs": []
    },
    {
      "id": "24e86fcb-f2d2-4899-a718-9f378fd41d48",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b9bcb5d9-4ab7-494a-a0ee-4d50553f2518"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:29:38.722306+00:00",
          "start_time": "2023-09-03T03:29:38.551604+00:00"
        }
      },
      "execution_count": null,
      "source": "# Checking for NaN values again in the imputed training and testing sets\nnan_train_imputed = pd.DataFrame(X_train).isna().sum().sum()\nnan_test_imputed = pd.DataFrame(X_test).isna().sum().sum()\n\nnan_train_imputed, nan_test_imputed",
      "outputs": []
    },
    {
      "id": "6b18c815-a62b-4cb9-8a21-8f2fa0dec58e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5ce1f619-87d9-4a2c-b07b-d767dd5ef242"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:29:56.166977+00:00",
          "start_time": "2023-09-03T03:29:55.985968+00:00"
        }
      },
      "execution_count": null,
      "source": "# Checking for NaN values in the target variable\nnan_y_train = y_train.isna().sum()\nnan_y_test = y_test.isna().sum()\n\nnan_y_train, nan_y_test",
      "outputs": []
    },
    {
      "id": "ad6e9dc6-1c3d-46a1-9141-6df3b61bb06d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "840efd27-8ef7-4b88-a9f6-db0dad181a61"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:30:22.126232+00:00",
          "start_time": "2023-09-03T03:30:21.436554+00:00"
        }
      },
      "execution_count": null,
      "source": "# Removing rows with NaN values in the target variable\nX_train = pd.DataFrame(X_train)[~y_train.isna()].values\ny_train = y_train.dropna().values\n\nX_test = pd.DataFrame(X_test)[~y_test.isna()].values\ny_test = y_test.dropna().values\n\n# Retraining the logistic regression model\nlogreg.fit(X_train, y_train)\n\n# Predicting on the test set\ny_pred = logreg.predict(X_test)\n\n# Evaluating the model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\naccuracy, classification_rep",
      "outputs": []
    },
    {
      "id": "c8893ec5-71be-422d-bf40-570ea95beaac",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b7a5bd6d-06eb-48c7-a2cc-a4eb6449633c"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:31:25.751096+00:00",
          "start_time": "2023-09-03T03:30:50.359145+00:00"
        }
      },
      "execution_count": null,
      "source": "# Resetting indices of y_train and y_test\ny_train = y_train.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n\n# Removing rows with NaN values in the target variable\nX_train = pd.DataFrame(X_train)[~y_train.isna()].values\ny_train = y_train.dropna().values\n\nX_test = pd.DataFrame(X_test)[~y_test.isna()].values\ny_test = y_test.dropna().values\n\n# Retraining the logistic regression model\nlogreg.fit(X_train, y_train)\n\n# Predicting on the test set\ny_pred = logreg.predict(X_test)\n\n# Evaluating the model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\naccuracy, classification_rep",
      "outputs": []
    },
    {
      "id": "c6ff7741-a5ab-438c-9b3f-6d3be83c1fca",
      "cell_type": "markdown",
      "source": "## Feature Importance using SHAP Values\n\nSHAP (SHapley Additive exPlanations) values provide a measure of the impact of each feature on the model's prediction for a given instance. By aggregating these values across all instances, we can get a sense of the overall importance of each feature in the model.\n\nLet's compute and visualize the SHAP values for our logistic regression model to understand which features are most influential in determining loan approvals.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "ac675413-a571-4858-a36f-62ddc6775e41",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9bdf6d19-0be4-4825-8e1d-0f0019701667"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:34:32.781031+00:00",
          "start_time": "2023-09-03T03:34:29.842377+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install -q shap",
      "outputs": []
    },
    {
      "id": "fe19677b-124a-42d7-8831-ee9e262cd644",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "aaa91d32-4180-4cd1-b276-3b37ba2dc46b"
        },
        "ExecuteTime": {
          "end_time": "2023-09-03T03:34:51.675685+00:00",
          "start_time": "2023-09-03T03:34:50.744801+00:00"
        }
      },
      "execution_count": null,
      "source": "import shap\n\n# Initializing the explainer\nexplainer = shap.LinearExplainer(logreg, X_train, feature_perturbation=\"interventional\")\n\n# Calculating SHAP values\nshap_values = explainer.shap_values(X_test)\n\n# Visualizing the SHAP values\nshap.summary_plot(shap_values, X_test, feature_names=features, plot_type=\"bar\")",
      "outputs": []
    }
  ]
}